# -*- coding: utf-8 -*-
"""MMDetection3D+KITTI_3Dí”„ë¡œì íŠ¸.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvkxKeWnfQdTW5Q6vIX821AaTI67IKWW

# 1. GPU/ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
"""

# Commented out IPython magic to ensure Python compatibility.
# # GPU/ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
# %%bash
# echo "========================================================================"
# echo "ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´"
# echo "========================================================================"
# echo ""
# echo "ğŸ“… í˜„ì¬ ì‹œê°„: $(date '+%Y-%m-%d %H:%M:%S')"
# echo ""
# echo "ğŸ”§ GPU ì •ë³´:"
# nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader
# echo ""
# echo "ğŸ”§ CUDA ë²„ì „:"
# nvcc --version 2>/dev/null | grep "release" || echo "nvcc not found"
# echo ""
# echo "ğŸ”§ Python ë²„ì „:"
# python --version
# echo ""
# echo "ğŸ”§ ë””ìŠ¤í¬ ê³µê°„:"
# df -h /content | tail -1 | awk '{print "   ì‚¬ìš©: " $3 " / " $2 " (" $5 " ì‚¬ìš©ì¤‘)"}'
# echo ""
# echo "========================================================================"

from google.colab import drive; drive.mount('/content/drive')

"""# 2. Miniconda + UV ì„¤ì¹˜"""

# Commented out IPython magic to ensure Python compatibility.
# # Miniconda + UV ì„¤ì¹˜
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ“¦ Miniconda + UV ì„¤ì¹˜"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# # Miniconda ì„¤ì¹˜
# if [ ! -d "/content/conda" ]; then
#     echo "1ï¸âƒ£ Miniconda ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜..."
#     wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
#     bash /tmp/miniconda.sh -b -p /content/conda > /dev/null 2>&1
#     rm /tmp/miniconda.sh
#     echo "   âœ… Miniconda ì„¤ì¹˜ ì™„ë£Œ"
# else
#     echo "1ï¸âƒ£ Miniconda ì´ë¯¸ ì„¤ì¹˜ë¨"
# fi
# 
# # UV ì„¤ì¹˜ (base í™˜ê²½ì—)
# if [ ! -f "/content/conda/bin/uv" ]; then
#     echo "2ï¸âƒ£ UV ì„¤ì¹˜..."
#     /content/conda/bin/pip install uv -q
#     echo "   âœ… UV ì„¤ì¹˜ ì™„ë£Œ"
# else
#     echo "2ï¸âƒ£ UV ì´ë¯¸ ì„¤ì¹˜ë¨"
# fi
# 
# echo ""
# echo "ğŸ“‹ UV ë²„ì „:"
# /content/conda/bin/uv --version
# 
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 3. Anaconda ToS ë™ì˜ + Python 3.10 í™˜ê²½ ìƒì„±"""

# Commented out IPython magic to ensure Python compatibility.
# # Anaconda ToS ë™ì˜ + Python 3.10 conda í™˜ê²½ ìƒì„±
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ Python 3.10 conda í™˜ê²½ ìƒì„±"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# source /content/conda/bin/activate
# 
# # Anaconda Terms of Service ë™ì˜
# echo "1ï¸âƒ£ Anaconda Terms of Service ë™ì˜..."
# conda config --set solver classic 2>/dev/null || true
# echo "y" | conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main 2>/dev/null || true
# echo "y" | conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r 2>/dev/null || true
# conda config --add channels conda-forge 2>/dev/null || true
# conda config --set channel_priority flexible 2>/dev/null || true
# echo "   âœ… ì„¤ì • ì™„ë£Œ"
# echo ""
# 
# if [ ! -d "/content/conda/envs/openmmlab" ]; then
#     echo "2ï¸âƒ£ openmmlab í™˜ê²½ ìƒì„± ì¤‘..."
#     conda create -n openmmlab python=3.10 -c conda-forge -y -q 2>&1 | tail -5
#     echo "   âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ"
# else
#     echo "2ï¸âƒ£ openmmlab í™˜ê²½ ì´ë¯¸ ì¡´ì¬"
# fi
# 
# echo ""
# echo "ğŸ“‹ Python ë²„ì „ í™•ì¸:"
# /content/conda/envs/openmmlab/bin/python --version
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 4. NumPy 1.26.4 ì„¤ì¹˜"""

# Commented out IPython magic to ensure Python compatibility.
# # NumPy 1.26.4 ì„¤ì¹˜
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ“¦ NumPy 1.26.4 ì„¤ì¹˜"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# PY=/content/conda/envs/openmmlab/bin/python
# UV=/content/conda/bin/uv
# 
# echo "1ï¸âƒ£ NumPy ì„¤ì¹˜ ì¤‘..."
# $UV pip install numpy==1.26.4 --python $PY -q
# 
# echo ""
# echo "ğŸ“‹ NumPy ë²„ì „ í™•ì¸:"
# $PY -c "import numpy; print(f'   NumPy: {numpy.__version__}')"
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 5. PyTorch 2.1.2 + CUDA 11.8 ì„¤ì¹˜"""

# Commented out IPython magic to ensure Python compatibility.
# # PyTorch 2.1.2 + CUDA 11.8 ì„¤ì¹˜
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ”¥ PyTorch 2.1.2 + CUDA 11.8 ì„¤ì¹˜"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# PY=/content/conda/envs/openmmlab/bin/python
# UV=/content/conda/bin/uv
# 
# echo "1ï¸âƒ£ PyTorch ì„¤ì¹˜ ì¤‘ (ì•½ 1ë¶„ ì†Œìš”)..."
# $UV pip install torch==2.1.2 torchvision==0.16.2 \
#     --index-url https://download.pytorch.org/whl/cu118 \
#     --python $PY -q
# 
# echo ""
# echo "ğŸ“‹ ì„¤ì¹˜ í™•ì¸:"
# $PY -c "
# import torch
# print(f'   PyTorch: {torch.__version__}')
# print(f'   CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
# if torch.cuda.is_available():
#     print(f'   GPU: {torch.cuda.get_device_name(0)}')
#     print(f'   CUDA ë²„ì „: {torch.version.cuda}')
# "
# 
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 6. OpenMMLab ìŠ¤íƒ ì„¤ì¹˜"""

# Commented out IPython magic to ensure Python compatibility.
# # OpenMMLab ìŠ¤íƒ ì„¤ì¹˜ (mmcv, mmengine, mmdet)
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ“¦ OpenMMLab ìŠ¤íƒ ì„¤ì¹˜"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# PY=/content/conda/envs/openmmlab/bin/python
# UV=/content/conda/bin/uv
# 
# echo "1ï¸âƒ£ mmcv 2.1.0 ì„¤ì¹˜ ì¤‘..."
# $UV pip install mmcv==2.1.0 \
#     -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1/index.html \
#     --python $PY -q
# echo "   âœ… mmcv ì„¤ì¹˜ ì™„ë£Œ"
# 
# echo "2ï¸âƒ£ mmengine ì„¤ì¹˜ ì¤‘..."
# $UV pip install mmengine==0.10.7 --python $PY -q
# echo "   âœ… mmengine ì„¤ì¹˜ ì™„ë£Œ"
# 
# echo "3ï¸âƒ£ mmdet ì„¤ì¹˜ ì¤‘..."
# $UV pip install mmdet==3.3.0 --python $PY -q
# echo "   âœ… mmdet ì„¤ì¹˜ ì™„ë£Œ"
# 
# echo ""
# echo "ğŸ“‹ ì„¤ì¹˜ í™•ì¸:"
# $PY -c "
# import mmcv
# import mmengine
# import mmdet
# print(f'   mmcv: {mmcv.__version__}')
# print(f'   mmengine: {mmengine.__version__}')
# print(f'   mmdet: {mmdet.__version__}')
# "
# 
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 7. MMDetection3D v1.4.0 ì„¤ì¹˜ + íŒ¨ì¹˜"""

# Commented out IPython magic to ensure Python compatibility.
# # MMDetection3D v1.4.0 ì„¤ì¹˜ + np.long íŒ¨ì¹˜
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ“¦ MMDetection3D v1.4.0 ì„¤ì¹˜"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# PY=/content/conda/envs/openmmlab/bin/python
# UV=/content/conda/bin/uv
# 
# cd /content
# 
# # í´ë¡ 
# if [ ! -d "mmdetection3d" ]; then
#     echo "1ï¸âƒ£ MMDetection3D í´ë¡  ì¤‘..."
#     git clone -b v1.4.0 --depth 1 https://github.com/open-mmlab/mmdetection3d.git 2>/dev/null
#     echo "   âœ… í´ë¡  ì™„ë£Œ"
# else
#     echo "1ï¸âƒ£ MMDetection3D ì´ë¯¸ ì¡´ì¬"
# fi
# 
# # ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì„¤ì¹˜
# echo "2ï¸âƒ£ ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
# cd mmdetection3d
# $UV pip install -r requirements/runtime.txt --python $PY -q 2>/dev/null || true
# $UV pip install "nuscenes-devkit==1.1.11" --python $PY -q 2>/dev/null || true
# echo "   âœ… ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ"
# 
# # np.long â†’ np.int64 íŒ¨ì¹˜
# echo "3ï¸âƒ£ NumPy í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì¤‘..."
# find /content/mmdetection3d -name "*.py" -exec grep -l "np\.long[^a-z]" {} \; 2>/dev/null | while read f; do
#     sed -i 's/np\.long\([^a-z]\)/np.int64\1/g' "$f"
# done
# echo "   âœ… np.long â†’ np.int64 íŒ¨ì¹˜ ì™„ë£Œ"
# 
# echo ""
# echo "ğŸ“‹ ì„¤ì¹˜ í™•ì¸:"
# export PYTHONPATH=/content/mmdetection3d:$PYTHONPATH
# $PY -c "import mmdet3d; print(f'   mmdet3d: {mmdet3d.__version__}')"
# 
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 8. numba 0.56.4 ì„¤ì¹˜ (PTX í˜¸í™˜ì„±) + ì˜ì¡´ì„± ê²€ì¦"""

# Commented out IPython magic to ensure Python compatibility.
# # numba ì„¤ì • (CUDA ë¹„í™œì„±í™” + CPU fallback)
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ”§ numba ì„¤ì • (CUDA í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘: $(date '+%H:%M:%S')"
# echo ""
# 
# PY=/content/conda/envs/openmmlab/bin/python
# UV=/content/conda/bin/uv
# export PYTHONPATH=/content/mmdetection3d:$PYTHONPATH
# 
# # numba 0.57.1 ì„¤ì¹˜ (ì¤‘ê°„ ë²„ì „)
# echo "1ï¸âƒ£ numba 0.57.1 + llvmlite 0.40.1 ì„¤ì¹˜..."
# $UV pip uninstall numba llvmlite -y --python $PY -q 2>/dev/null || true
# $UV pip install numba==0.57.1 llvmlite==0.40.1 --python $PY -q
# echo "   âœ… numba 0.57.1 ì„¤ì¹˜ ì™„ë£Œ"
# 
# # numba ìºì‹œ ì‚­ì œ
# echo ""
# echo "2ï¸âƒ£ numba ìºì‹œ ì‚­ì œ..."
# rm -rf ~/.cache/numba
# rm -rf /root/.cache/numba
# rm -rf /content/mmdetection3d/mmdet3d/evaluation/functional/kitti_utils/__pycache__
# find /content/mmdetection3d -name "*.nbc" -delete 2>/dev/null || true
# find /content/mmdetection3d -name "*.nbi" -delete 2>/dev/null || true
# echo "   âœ… ìºì‹œ ì‚­ì œ ì™„ë£Œ"
# 
# # âš ï¸ í•µì‹¬: numba CUDA ë¹„í™œì„±í™” ì„¤ì • íŒŒì¼ ìƒì„±
# echo ""
# echo "3ï¸âƒ£ numba CUDA ë¹„í™œì„±í™” ì„¤ì •..."
# mkdir -p /content/conda/envs/openmmlab/etc/conda/activate.d
# cat > /content/conda/envs/openmmlab/etc/conda/activate.d/env_vars.sh << 'ENVEOF'
# #!/bin/bash
# export NUMBA_DISABLE_CUDA=1
# ENVEOF
# chmod +x /content/conda/envs/openmmlab/etc/conda/activate.d/env_vars.sh
# echo "   âœ… NUMBA_DISABLE_CUDA=1 ì„¤ì • ì™„ë£Œ"
# 
# echo ""
# echo "4ï¸âƒ£ ì˜ì¡´ì„± í™•ì¸:"
# $PY << 'EOF'
# import os
# os.environ['NUMBA_DISABLE_CUDA'] = '1'  # Python ë‚´ì—ì„œë„ ì„¤ì •
# 
# import sys
# print(f"   Python: {sys.version.split()[0]}")
# 
# import numpy as np
# print(f"   NumPy: {np.__version__}")
# 
# import torch
# print(f"   PyTorch: {torch.__version__}")
# print(f"   CUDA available: {torch.cuda.is_available()}")
# 
# import mmcv
# print(f"   mmcv: {mmcv.__version__}")
# 
# import mmengine
# print(f"   mmengine: {mmengine.__version__}")
# 
# import mmdet
# print(f"   mmdet: {mmdet.__version__}")
# 
# import mmdet3d
# print(f"   mmdet3d: {mmdet3d.__version__}")
# 
# import numba
# print(f"   numba: {numba.__version__}")
# print(f"   NUMBA_DISABLE_CUDA: {os.environ.get('NUMBA_DISABLE_CUDA', 'not set')}")
# 
# import llvmlite
# print(f"   llvmlite: {llvmlite.__version__}")
# 
# if torch.cuda.is_available():
#     print(f"\n   GPU: {torch.cuda.get_device_name(0)}")
#     print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
# EOF
# 
# echo ""
# echo "ğŸ“… ì™„ë£Œ: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 9. KITTI ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""

# Commented out IPython magic to ensure Python compatibility.
# # KITTI ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
# %%bash
# set -e
# 
# echo "========================================================================"
# echo "ğŸ“¥ KITTI 3D ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"
# echo "========================================================================"
# echo "ğŸ“… ì‹œì‘ ì‹œê°„: $(date '+%H:%M:%S')"
# echo ""
# 
# # aria2 ì„¤ì¹˜
# echo "1ï¸âƒ£ aria2 ì„¤ì¹˜ ì¤‘..."
# apt-get update -qq && apt-get install -y aria2 -qq 2>/dev/null
# echo "   âœ… aria2 ì„¤ì¹˜ ì™„ë£Œ"
# echo ""
# 
# DATA_ROOT=/content/data/kitti
# mkdir -p ${DATA_ROOT}
# cd ${DATA_ROOT}
# 
# # ì´ë¯¸ ì™„ë£Œ í™•ì¸
# if [ -d "training/velodyne" ] && [ $(ls training/velodyne/*.bin 2>/dev/null | wc -l) -gt 7000 ]; then
#     echo "âœ… KITTI ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤!"
#     echo "   Training: $(ls training/velodyne/*.bin | wc -l) samples"
#     echo "   Testing: $(ls testing/velodyne/*.bin 2>/dev/null | wc -l) samples"
#     exit 0
# fi
# 
# echo "ğŸ“‹ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼:"
# echo "   1. data_object_calib.zip (~16MB)"
# echo "   2. data_object_label_2.zip (~5MB)"
# echo "   3. data_object_image_2.zip (~12GB)"
# echo "   4. data_object_velodyne.zip (~29GB)"
# echo ""
# echo "â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 20-40ë¶„"
# echo ""
# echo "========================================================================"
# 
# download_and_extract() {
#     local filename=$1
#     local desc=$2
#     local url="https://s3.eu-central-1.amazonaws.com/avg-kitti/${filename}"
# 
#     echo ""
#     echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
#     echo "ğŸ”½ ${filename} - ${desc}"
#     echo "   ì‹œì‘: $(date '+%H:%M:%S')"
# 
#     aria2c -x 16 -s 16 -k 1M \
#         --summary-interval=60 \
#         --console-log-level=notice \
#         --download-result=hide \
#         "${url}" -o "${filename}" 2>&1 | grep -E "(\[#|Download)" || true
# 
#     if [ -f "${filename}" ]; then
#         echo "   ğŸ“¦ ì••ì¶• í•´ì œ ì¤‘..."
#         unzip -q -o "${filename}"
#         rm -f "${filename}"
#         echo "   âœ… ì™„ë£Œ: $(date '+%H:%M:%S')"
#     fi
# }
# 
# echo ""
# echo "2ï¸âƒ£ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘"
# 
# download_and_extract "data_object_calib.zip" "Calibration"
# download_and_extract "data_object_label_2.zip" "Labels"
# download_and_extract "data_object_image_2.zip" "RGB Images"
# download_and_extract "data_object_velodyne.zip" "LiDAR Point Cloud"
# 
# echo ""
# echo "3ï¸âƒ£ ImageSets ë‹¤ìš´ë¡œë“œ"
# mkdir -p ImageSets
# for split in train val test; do
#     if [ ! -f "ImageSets/${split}.txt" ]; then
#         wget -q https://raw.githubusercontent.com/traveller59/second.pytorch/master/second/data/ImageSets/${split}.txt -O ImageSets/${split}.txt
#     fi
#     echo "   âœ… ${split}.txt: $(wc -l < ImageSets/${split}.txt) samples"
# done
# 
# echo ""
# echo "========================================================================"
# echo "âœ… KITTI ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!"
# echo "========================================================================"
# echo "ğŸ“Š ë°ì´í„° í†µê³„:"
# echo "   Training: $(ls training/velodyne/*.bin 2>/dev/null | wc -l) samples"
# echo "   Testing: $(ls testing/velodyne/*.bin 2>/dev/null | wc -l) samples"
# echo "   ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: $(du -sh ${DATA_ROOT} | cut -f1)"
# echo "ğŸ“… ì™„ë£Œ ì‹œê°„: $(date '+%H:%M:%S')"
# echo "========================================================================"

"""# 11. ë°ì´í„° ì „ì²˜ë¦¬"""

# KITTI ë°ì´í„° ì „ì²˜ë¦¬ (ì‹¤ì‹œê°„ ë¡œê¹…)
import subprocess
import os
from datetime import datetime

print("=" * 70)
print("ğŸ”„ KITTI ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 70)
print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
print()

# ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
os.makedirs('/content/mmdetection3d/data', exist_ok=True)
link_path = '/content/mmdetection3d/data/kitti'
if os.path.islink(link_path):
    os.remove(link_path)
elif os.path.exists(link_path):
    import shutil
    shutil.rmtree(link_path)
os.symlink('/content/data/kitti', link_path)
print("ğŸ”— ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì™„ë£Œ")
print()

print("ğŸ“‹ ìƒì„±í•  íŒŒì¼:")
print("   â€¢ kitti_infos_train.pkl")
print("   â€¢ kitti_infos_val.pkl")
print("   â€¢ kitti_dbinfos_train.pkl")
print("   â€¢ training/velodyne_reduced/")
print()
print("â³ ì „ì²˜ë¦¬ ì‹œì‘ (ì•½ 10-15ë¶„ ì†Œìš”)...")
print("-" * 70)

env = os.environ.copy()
env['PYTHONPATH'] = '/content/mmdetection3d:' + env.get('PYTHONPATH', '')
env['PYTHONUNBUFFERED'] = '1'

cmd = [
    '/content/conda/envs/openmmlab/bin/python', '-u',
    '/content/mmdetection3d/tools/create_data.py', 'kitti',
    '--root-path', '/content/data/kitti',
    '--out-dir', '/content/data/kitti',
    '--extra-tag', 'kitti',
    '--workers', '2'
]

process = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,
    env=env,
    text=True,
    bufsize=1,
    cwd='/content/mmdetection3d'
)

for line in iter(process.stdout.readline, ''):
    print(line, end='', flush=True)

process.wait()

print("-" * 70)
print()
print("=" * 70)
print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
print("=" * 70)

import glob
print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
for pkl in sorted(glob.glob('/content/data/kitti/*.pkl')):
    size = os.path.getsize(pkl) / 1024 / 1024
    print(f"   {os.path.basename(pkl)}: {size:.1f} MB")

reduced_dir = '/content/data/kitti/training/velodyne_reduced'
if os.path.exists(reduced_dir):
    count = len([f for f in os.listdir(reduced_dir) if f.endswith('.bin')])
    print(f"   velodyne_reduced/: {count} files")

print(f"\nğŸ“… ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
print("=" * 70)

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# set -e
# 
# DRIVE_DIR="/content/drive/MyDrive/Colab Notebooks/3d_project"
# 
# cp "$DRIVE_DIR/pseudo_velodyne.tar.gz" /content/pseudo_velodyne.tar.gz
# # pth íŒŒì¼ëª… ê¸¸ë©´ ì™€ì¼ë“œì¹´ë“œë¡œ
# cp "$DRIVE_DIR/"*.pth /content/mvxnet_teacher.pth
# 
# ls -lah /content/pseudo_velodyne.tar.gz
# ls -lah /content/mvxnet_teacher.pth
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# set -e
# mkdir -p /content/data/kitti/training/pseudo_velodyne
# 
# tar -xzf /content/pseudo_velodyne.tar.gz -C /content/data/kitti/training/pseudo_velodyne
# 
# # 000000.binì´ ë°”ë¡œ ë³´ì´ëŠ”ì§€ í™•ì¸
# ls -lah /content/data/kitti/training/pseudo_velodyne | head
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# set -e
# 
# PSEUDO_ROOT="/content/data/kitti/training/pseudo_velodyne"
# TAR_PATH="/content/pseudo_velodyne.tar.gz"
# 
# # tarì—ì„œ ì²« ë²ˆì§¸ .bin ê²½ë¡œë¥¼ ë³´ê³ , í´ë” depth ê³„ì‚°
# FIRST_BIN=$(tar -tzf "$TAR_PATH" | grep -m1 '\.bin$' || true)
# if [ -z "$FIRST_BIN" ]; then
#   echo "âŒ tar ì•ˆì—ì„œ .bin íŒŒì¼ì„ ëª» ì°¾ì•˜ì–´. ì••ì¶• íŒŒì¼ì´ ë§ëŠ”ì§€ í™•ì¸ í•„ìš”"
#   exit 1
# fi
# 
# STRIP=$(echo "$FIRST_BIN" | awk -F/ '{print NF-1}')  # ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ ê°œìˆ˜
# echo "âœ… FIRST_BIN = $FIRST_BIN"
# echo "âœ… STRIP_COMPONENTS = $STRIP"
# 
# # ë‹¤ì‹œ ê¹”ë”í•˜ê²Œ í’€ê¸°
# rm -rf "$PSEUDO_ROOT"
# mkdir -p "$PSEUDO_ROOT"
# 
# if [ "$STRIP" -gt 0 ]; then
#   tar -xzf "$TAR_PATH" -C "$PSEUDO_ROOT" --strip-components="$STRIP"
# else
#   tar -xzf "$TAR_PATH" -C "$PSEUDO_ROOT"
# fi
# 
# echo "---- after extract (top) ----"
# ls -lah "$PSEUDO_ROOT" | head
# echo "---- find 000000.bin ----"
# find "$PSEUDO_ROOT" -maxdepth 2 -name "000000.bin" -print | head -n 5
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# set -e
# ls -lah /content/mvxnet_teacher.pth
# ls -lah /content/data/kitti/training/pseudo_velodyne/000000.bin
# ls -lah /content/data/kitti/training/velodyne/000000.bin
# ls -lah /content/data/kitti/training/image_2/000000.png
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# set -e
# 
# PY="/content/conda/envs/openmmlab/bin/python"
# MMDET3D="/content/mmdetection3d"
# KITTI="/content/data/kitti"
# 
# CFG="$MMDET3D/configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py"
# TEACHER_CKPT="/content/mvxnet_teacher.pth"
# STUDENT_CKPT="$TEACHER_CKPT"
# 
# PSEUDO_DIR="$KITTI/training/pseudo_velodyne"
# FRAME_ID="000000"
# STAGE="middle"
# OUT_DIR="/content/bev_out"
# mkdir -p "$OUT_DIR"
# 
# export PYTHONPATH="/content/mmdetection3d:$PYTHONPATH"
# 
# cat > /content/extract_bev_mvxnet.py <<'PY'
# import os, re, copy, argparse, tempfile
# import numpy as np
# import torch
# 
# from mmengine.config import Config
# from mmengine.dataset import pseudo_collate
# from mmdet3d.apis import init_model
# from mmdet3d.registry import DATASETS
# 
# def zfill6(x) -> str:
#     s = str(x)
#     m = re.findall(r'\d+', s)
#     if m:
#         s = m[-1]
#     s = s[-6:]
#     return s.zfill(6)
# 
# def ensure_kitti_bin4(bin_path: str) -> str:
#     arr = np.fromfile(bin_path, dtype=np.float32)
#     if arr.size % 4 == 0:
#         return bin_path
#     if arr.size % 3 != 0:
#         raise ValueError(f"bin float count not divisible by 3 or 4: {bin_path} (floats={arr.size})")
#     pts3 = arr.reshape(-1, 3)
#     pts4 = np.concatenate([pts3, np.zeros((pts3.shape[0], 1), dtype=np.float32)], axis=1)
#     tmp = tempfile.NamedTemporaryFile(suffix=".bin", delete=False)
#     pts4.astype(np.float32).tofile(tmp.name)
#     return tmp.name
# 
# def set_lidar_path(data_info: dict, new_abs_path: str):
#     if 'lidar_points' in data_info and isinstance(data_info['lidar_points'], dict):
#         if 'lidar_path' in data_info['lidar_points']:
#             data_info['lidar_points']['lidar_path'] = new_abs_path
#             return
#     if 'pts_filename' in data_info:
#         data_info['pts_filename'] = new_abs_path
#         return
#     for k, v in data_info.items():
#         if isinstance(v, dict):
#             for kk in list(v.keys()):
#                 if 'lidar' in kk and 'path' in kk:
#                     v[kk] = new_abs_path
#                     return
#     raise KeyError("Could not find lidar path key in data_info")
# 
# def find_index_by_frame(dataset, frame_id: str) -> int:
#     target = zfill6(frame_id)
#     for i in range(len(dataset)):
#         info = dataset.get_data_info(i)
#         for key in ['sample_idx', 'image_idx', 'idx', 'frame_id']:
#             if key in info and zfill6(info[key]) == target:
#                 return i
#         try:
#             if 'lidar_points' in info and 'lidar_path' in info['lidar_points']:
#                 base = os.path.basename(info['lidar_points']['lidar_path'])
#                 if zfill6(os.path.splitext(base)[0]) == target:
#                     return i
#         except Exception:
#             pass
#         if 'images' in info and isinstance(info['images'], dict):
#             for caminfo in info['images'].values():
#                 p = caminfo.get('img_path', '')
#                 if p:
#                     base = os.path.basename(p)
#                     if zfill6(os.path.splitext(base)[0]) == target:
#                         return i
#     raise ValueError(f"Frame id {target} not found in dataset infos")
# 
# def pick_tensor(x):
#     if isinstance(x, torch.Tensor):
#         return x
#     if isinstance(x, dict):
#         for k in ['spatial_features', 'out', 'features', 'feat']:
#             if k in x and isinstance(x[k], torch.Tensor):
#                 return x[k]
#         for v in x.values():
#             if isinstance(v, torch.Tensor):
#                 return v
#     if isinstance(x, (list, tuple)):
#         for v in x:
#             t = pick_tensor(v)
#             if t is not None:
#                 return t
#     return None
# 
# def get_stage_module(model, stage: str):
#     if stage == 'middle' and hasattr(model, 'pts_middle_encoder'):
#         return model.pts_middle_encoder
#     if stage == 'backbone' and hasattr(model, 'pts_backbone'):
#         return model.pts_backbone
#     if stage == 'neck' and hasattr(model, 'pts_neck'):
#         return model.pts_neck
#     for name in ['pts_middle_encoder', 'pts_backbone', 'pts_neck']:
#         if hasattr(model, name):
#             return getattr(model, name)
#     raise AttributeError("No pts_* module found on model")
# 
# def to_meta(ds):
#     # Det3DDataSample/BaseDataElement -> metainfo dict
#     if isinstance(ds, dict):
#         return ds
#     if hasattr(ds, 'metainfo') and isinstance(ds.metainfo, dict):
#         return ds.metainfo
#     if hasattr(ds, 'to_dict'):
#         d = ds.to_dict()
#         return d if isinstance(d, dict) else {}
#     # ìµœí›„: attribute dict
#     return {}
# 
# def main():
#     ap = argparse.ArgumentParser()
#     ap.add_argument('--config', required=True)
#     ap.add_argument('--checkpoint', required=True)
#     ap.add_argument('--kitti_root', required=True)
#     ap.add_argument('--frame_id', required=True)
#     ap.add_argument('--lidar_override', default=None)
#     ap.add_argument('--stage', default='middle', choices=['middle','backbone','neck'])
#     ap.add_argument('--out', required=True)
#     ap.add_argument('--device', default='cuda:0')
#     args = ap.parse_args()
# 
#     cfg = Config.fromfile(args.config)
#     cfg.test_dataloader.dataset = copy.deepcopy(cfg.test_dataloader.dataset)
#     cfg.test_dataloader.dataset['data_root'] = args.kitti_root + '/'
# 
#     model = init_model(cfg, args.checkpoint, device=args.device)
#     model.eval()
# 
#     # âœ… (ì•ˆì „) point_fusionì´ Det3DDataSampleì„ ë°›ì•„ë„ metainfoë¡œ ë³€í™˜í•˜ë„ë¡ íŒ¨ì¹˜
#     try:
#         import mmdet3d.models.layers.fusion_layers.point_fusion as pf
#         _orig = pf.get_proj_mat_by_coord_type
#         def _patched(img_meta, coord_type):
#             if not isinstance(img_meta, dict) and hasattr(img_meta, 'metainfo'):
#                 img_meta = img_meta.metainfo
#             return _orig(img_meta, coord_type)
#         pf.get_proj_mat_by_coord_type = _patched
#     except Exception as e:
#         print("[WARN] point_fusion patch skipped:", repr(e))
# 
#     dataset = DATASETS.build(cfg.test_dataloader.dataset)
#     idx = find_index_by_frame(dataset, args.frame_id)
#     info = copy.deepcopy(dataset.get_data_info(idx))
# 
#     if args.lidar_override is not None:
#         fixed = ensure_kitti_bin4(args.lidar_override)
#         set_lidar_path(info, fixed)
# 
#     packed = dataset.pipeline(info)
#     batch = pseudo_collate([packed])
#     batch = model.data_preprocessor(batch, training=False)
# 
#     inputs = batch['inputs']
#     data_samples = batch['data_samples']
# 
#     # âœ… í•µì‹¬: Det3DDataSample ë¦¬ìŠ¤íŠ¸ -> metainfo dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ë„˜ê¹€
#     img_metas = [to_meta(ds) for ds in data_samples]
# 
#     # hookìœ¼ë¡œ BEV ì¡ê¸°
#     cache = {}
#     module = get_stage_module(model, args.stage)
# 
#     def hook_fn(_m, _inp, _out):
#         cache['out'] = _out
# 
#     h = module.register_forward_hook(hook_fn)
#     with torch.no_grad():
#         # âœ… data_samples ëŒ€ì‹  img_metas(dict list)ë¥¼ ë„˜ê¸´ë‹¤
#         _ = model.extract_feat(inputs, img_metas)
#     h.remove()
# 
#     bev = pick_tensor(cache.get('out', None))
#     if bev is None:
#         raise RuntimeError("Could not capture BEV tensor from hook output.")
# 
#     os.makedirs(os.path.dirname(args.out), exist_ok=True)
#     torch.save({'bev': bev.detach().cpu(),
#                 'frame_id': zfill6(args.frame_id),
#                 'stage': args.stage,
#                 'checkpoint': os.path.basename(args.checkpoint),
#                 'lidar_override': args.lidar_override}, args.out)
# 
#     print(f"[OK] Saved: {args.out}")
#     print(f"BEV shape: {tuple(bev.shape)}")
# 
# if __name__ == "__main__":
#     main()
# PY
# 
# echo "âœ… Teacher BEV..."
# $PY /content/extract_bev_mvxnet.py \
#   --config "$CFG" \
#   --checkpoint "$TEACHER_CKPT" \
#   --kitti_root "$KITTI" \
#   --frame_id "$FRAME_ID" \
#   --stage "$STAGE" \
#   --out "$OUT_DIR/teacher_${FRAME_ID}_${STAGE}.pt"
# 
# echo "âœ… Student BEV (Pseudo override)..."
# $PY /content/extract_bev_mvxnet.py \
#   --config "$CFG" \
#   --checkpoint "$STUDENT_CKPT" \
#   --kitti_root "$KITTI" \
#   --frame_id "$FRAME_ID" \
#   --lidar_override "$PSEUDO_DIR/$FRAME_ID.bin" \
#   --stage "$STAGE" \
#   --out "$OUT_DIR/student_${FRAME_ID}_${STAGE}.pt"
# 
# echo "âœ… DONE:"
# ls -lah "$OUT_DIR"
#

import os
import torch
import numpy as np
import matplotlib.pyplot as plt

TEACHER_PT = "/content/bev_out/teacher_000000_middle.pt"
STUDENT_PT = "/content/bev_out/student_000000_middle.pt"
OUT_DIR = "/content/bev_vis"
os.makedirs(OUT_DIR, exist_ok=True)

def bev_to_map(bev, mode="mean_abs"):
    # bev: (1,C,H,W) or (C,H,W)
    if bev.ndim == 4:
        bev = bev[0]
    if mode == "mean_abs":
        m = bev.abs().mean(dim=0)
    elif mode == "l2":
        m = torch.sqrt((bev ** 2).sum(dim=0) + 1e-12)
    elif mode == "max_abs":
        m = bev.abs().max(dim=0).values
    else:
        raise ValueError("mode must be one of: mean_abs, l2, max_abs")
    return m

def normalize_01(x):
    x = x.detach().cpu().float()
    x = x - x.min()
    x = x / (x.max() + 1e-12)
    return x

t = torch.load(TEACHER_PT)["bev"].float()
s = torch.load(STUDENT_PT)["bev"].float()

t_map = normalize_01(bev_to_map(t, "mean_abs"))
s_map = normalize_01(bev_to_map(s, "mean_abs"))
d_map = normalize_01((t_map - s_map).abs())

# Teacher
plt.figure()
plt.title("Teacher BEV (mean_abs)")
plt.imshow(t_map.numpy())
plt.axis("off")
plt.savefig(os.path.join(OUT_DIR, "teacher_mean_abs.png"), dpi=200, bbox_inches="tight")
plt.show()

# Student
plt.figure()
plt.title("Student BEV (mean_abs)")
plt.imshow(s_map.numpy())
plt.axis("off")
plt.savefig(os.path.join(OUT_DIR, "student_mean_abs.png"), dpi=200, bbox_inches="tight")
plt.show()

# Diff
plt.figure()
plt.title("Diff |Teacher-Student| (mean_abs)")
plt.imshow(d_map.numpy())
plt.axis("off")
plt.savefig(os.path.join(OUT_DIR, "diff_mean_abs.png"), dpi=200, bbox_inches="tight")
plt.show()

print("Saved to:", OUT_DIR)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile auto_train.py
# import os
# import shutil
# import numpy as np
# import glob
# import sys
# # ê²½ë¡œ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œìŠ¤í…œ ê²½ë¡œ ì¶”ê°€
# sys.path.append('/content/mmdetection3d')
# 
# from mmdet3d.utils import register_all_modules
# from mmengine.config import Config
# from mmengine.runner import Runner
# from tqdm import tqdm
# 
# # =================================================================
# # [ì„¤ì •] ZIP íŒŒì¼ ê²½ë¡œ (ì—¬ê¸°ë¥¼ ì‚¬ìš©ìë‹˜ ê²½ë¡œë¡œ!)
# # =================================================================
# ZIP_PATH = '/content/drive/MyDrive/Kitti_3D_Project/glpn_output.zip'
# SOURCE_PATH = '/content/drive/MyDrive/Kitti_3D_Project/glpn_output' # í´ë”ì¼ ê²½ìš° ëŒ€ë¹„
# TEACHER_PATH = '/content/drive/MyDrive/Kitti_3D_Project/checkpoints/mxvnet_best.pth'
# DATA_ROOT = '/content/kitti_pseudo_lidar'
# VELODYNE_DIR = os.path.join(DATA_ROOT, 'training/velodyne_reduced')
# 
# def main():
#     print(f"ğŸš€ [1ë‹¨ê³„] ë°ì´í„° ì°¾ê¸° ì‹œì‘...")
# 
#     # 1. íŒŒì¼ ì°¾ê¸° (Zipì´ë“  í´ë”ë“  ì‹¹ ë‹¤ ë’¤ì§)
#     # í´ë” ê²€ìƒ‰
#     files = glob.glob(os.path.join(SOURCE_PATH, '**', '*.npy'), recursive=True) + \
#             glob.glob(os.path.join(SOURCE_PATH, '**', '*.bin'), recursive=True)
# 
#     # ë§Œì•½ í´ë”ì— ì—†ìœ¼ë©´ Zip í•´ì œ ì‹œë„ (í˜¹ì‹œ ëª¨ë¥´ë‹ˆ)
#     if len(files) == 0 and os.path.exists(ZIP_PATH):
#         import zipfile
#         print("   -> í´ë”ì— íŒŒì¼ì´ ì—†ì–´ ZIP í•´ì œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
#         temp_zip_dir = '/content/temp_zip_extract'
#         with zipfile.ZipFile(ZIP_PATH, 'r') as z:
#             z.extractall(temp_zip_dir)
#         files = glob.glob(os.path.join(temp_zip_dir, '**', '*.npy'), recursive=True) + \
#                 glob.glob(os.path.join(temp_zip_dir, '**', '*.bin'), recursive=True)
# 
#     if len(files) == 0:
#         print("âŒ [ì‹¤íŒ¨] ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
#         return
# 
#     print(f"   âœ… ë°ì´í„° íŒŒì¼ {len(files)}ê°œ í™•ë³´! ë³€í™˜ ì‹œì‘...")
# 
#     # 2. ë°ì´í„° ë³€í™˜ (.bin)
#     if os.path.exists(VELODYNE_DIR): shutil.rmtree(VELODYNE_DIR)
#     os.makedirs(VELODYNE_DIR, exist_ok=True)
# 
#     for src in tqdm(files):
#         try:
#             if src.endswith('.npy'):
#                 points = np.load(src)
#             else:
#                 points = np.fromfile(src, dtype=np.float32).reshape(-1, 4)
# 
#             # .bin ì €ì¥
#             name = os.path.splitext(os.path.basename(src))[0] + '.bin'
#             dst = os.path.join(VELODYNE_DIR, name)
#             points.astype(np.float32).tofile(dst)
#         except Exception as e:
#             print(f"   âš ï¸ íŒŒì¼ ì—ëŸ¬ (ê±´ë„ˆëœ€): {src}")
# 
#     # 3. í•™ìŠµ ì •ë³´ ìƒì„± (create_data.py í˜¸ì¶œ)
#     print("\nğŸš€ [2ë‹¨ê³„] ì •ë³´ íŒŒì¼(.pkl) ìƒì„± ì¤‘...")
#     os.system(f"python /content/mmdetection3d/tools/create_data.py kitti --root-path {DATA_ROOT} --out-dir {DATA_ROOT} --extra-tag kitti")
# 
#     # 4. í•™ìŠµ ì‹œì‘
#     print("\nğŸš€ [3ë‹¨ê³„] í•™ìŠµ ì‹œì‘ (Config ë¡œë“œ)")
#     register_all_modules(init_default_scope=True)
#     cfg = Config.fromfile('/content/mmdetection3d/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py')
# 
#     # ê²½ë¡œ ê°•ì œ ì—°ê²°
#     cfg.data_root = DATA_ROOT
#     cfg.train_dataloader.dataset.data_root = DATA_ROOT
#     cfg.train_dataloader.dataset.data_prefix = dict(pts='training/velodyne_reduced')
#     cfg.train_dataloader.dataset.ann_file = 'kitti_infos_train.pkl'
# 
#     cfg.val_dataloader.dataset.data_root = DATA_ROOT
#     cfg.val_dataloader.dataset.data_prefix = dict(pts='training/velodyne_reduced')
#     cfg.val_dataloader.dataset.ann_file = 'kitti_infos_val.pkl'
# 
#     cfg.test_dataloader.dataset.data_root = DATA_ROOT
#     cfg.test_dataloader.dataset.data_prefix = dict(pts='training/velodyne_reduced')
#     cfg.test_dataloader.dataset.ann_file = 'kitti_infos_val.pkl'
# 
#     # íŒŒë¼ë¯¸í„°
#     cfg.train_dataloader.batch_size = 4
#     cfg.train_cfg.max_epochs = 50
#     cfg.work_dir = '/content/drive/MyDrive/Kitti_3D_Project/work_dirs/final_kd_run'
# 
#     # Teacher ëª¨ë¸ ë¡œë“œ
#     if os.path.exists(TEACHER_PATH):
#         print(f"   âœ… Teacher ëª¨ë¸ ë¡œë“œ: {TEACHER_PATH}")
#         cfg.load_from = TEACHER_PATH
# 
#     # ì‹¤í–‰
#     runner = Runner.from_cfg(cfg)
#     runner.train()
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile run_all_in_one.py
# import os
# import zipfile
# import numpy as np
# import torch
# from mmengine.config import Config
# from mmengine.runner import Runner
# from mmdet3d.utils import register_all_modules
# from tqdm import tqdm
# import shutil
# 
# # =================================================================
# # 1. [ì—¬ê¸°ë§Œ ìˆ˜ì •] ê²½ë¡œ ì„¤ì •
# # =================================================================
# # (1) GLPN ê²°ê³¼ë¬¼ (Zip íŒŒì¼) ê²½ë¡œ
# ZIP_PATH = '/content/drive/MyDrive/Colab Notebooks/3d_project/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class-8963258a.pth'
# 
# # (2) Teacher ëª¨ë¸ (MXVNet) ê²½ë¡œ
# TEACHER_PATH = '/content/drive/MyDrive/Colab Notebooks/3d_project/pseudo_velodyne.tar.gz'
# 
# # (3) ìŠ¤ì¼€ì¼ë§ ê°’ (3Dbbox í”„ë¡œì íŠ¸ì—ì„œ êµ¬í•œ ê°’, ëª¨ë¥´ë©´ 1.0)
# SCALE_FACTOR = 1.0
# # =================================================================
# 
# # ê³ ì • ê²½ë¡œ (ìˆ˜ì • ë¶ˆí•„ìš”)
# DATA_ROOT = '/content/kitti_pseudo_lidar'
# VELODYNE_DIR = os.path.join(DATA_ROOT, 'training/velodyne_reduced')
# WORK_DIR = '/content/drive/MyDrive/Kitti_3D_Project/work_dirs/final_kd_run'
# 
# def step1_prepare_data():
#     print(f"\nğŸš€ [1ë‹¨ê³„] ë°ì´í„° ì••ì¶• í•´ì œ ë° ìŠ¤ì¼€ì¼ë§ ì‹œì‘...")
# 
#     # ì´ˆê¸°í™”
#     if os.path.exists(VELODYNE_DIR): shutil.rmtree(VELODYNE_DIR)
#     os.makedirs(VELODYNE_DIR, exist_ok=True)
# 
#     # ì„ì‹œ ì••ì¶• í•´ì œ
#     temp_dir = '/content/temp_unzip'
#     if os.path.exists(temp_dir): shutil.rmtree(temp_dir)
#     os.makedirs(temp_dir, exist_ok=True)
# 
#     print(f"   - Zip íŒŒì¼: {ZIP_PATH}")
#     with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
#         zip_ref.extractall(temp_dir)
# 
#     # íŒŒì¼ ë³€í™˜ (npy/bin -> scaled bin)
#     files = []
#     for r, d, f in os.walk(temp_dir):
#         for file in f:
#             if file.endswith(('.npy', '.bin')):
#                 files.append(os.path.join(r, file))
# 
#     print(f"   - ë³€í™˜ ëŒ€ìƒ: {len(files)}ê°œ íŒŒì¼ (ìŠ¤ì¼€ì¼: x{SCALE_FACTOR})")
# 
#     for src in tqdm(files):
#         # ë¡œë“œ
#         if src.endswith('.npy'):
#             points = np.load(src)
#         else:
#             points = np.fromfile(src, dtype=np.float32).reshape(-1, 4)
# 
#         # ìŠ¤ì¼€ì¼ë§ (XYZ ì¢Œí‘œ ë³´ì •)
#         points[:, :3] *= SCALE_FACTOR
# 
#         # ì €ì¥ (.bin)
#         name = os.path.splitext(os.path.basename(src))[0] + '.bin'
#         dst = os.path.join(VELODYNE_DIR, name)
#         points.astype(np.float32).tofile(dst)
# 
#     print("   âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
# 
# def step2_create_infos():
#     print(f"\nğŸš€ [2ë‹¨ê³„] MMDetection3Dìš© ì •ë³´ íŒŒì¼(.pkl) ìƒì„±")
#     # create_data.py ì‹¤í–‰ê³¼ ë™ì¼í•œ íš¨ê³¼
#     # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ cmd í˜¸ì¶œ
#     cmd = f"python /content/mmdetection3d/tools/create_data.py kitti --root-path {DATA_ROOT} --out-dir {DATA_ROOT} --extra-tag kitti"
#     os.system(cmd)
#     print("   âœ… ì •ë³´ ìƒì„± ì™„ë£Œ!")
# 
# def step3_train():
#     print(f"\nğŸš€ [3ë‹¨ê³„] KD í•™ìŠµ ì‹œì‘ (Teacher: MXVNet)")
#     register_all_modules(init_default_scope=True)
# 
#     # Config ë¡œë“œ
#     cfg = Config.fromfile('/content/mmdetection3d/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py')
# 
#     # ê²½ë¡œ ê°•ì œ ì—°ê²°
#     cfg.data_root = DATA_ROOT
#     cfg.train_dataloader.dataset.data_root = DATA_ROOT
#     cfg.train_dataloader.dataset.data_prefix = dict(pts='training/velodyne_reduced')
#     cfg.train_dataloader.dataset.ann_file = 'kitti_infos_train.pkl'
# 
#     cfg.val_dataloader.dataset.data_root = DATA_ROOT
#     cfg.val_dataloader.dataset.data_prefix = dict(pts='training/velodyne_reduced')
#     cfg.val_dataloader.dataset.ann_file = 'kitti_infos_val.pkl'
# 
#     # í•™ìŠµ ì„¤ì •
#     cfg.train_dataloader.batch_size = 4
#     cfg.train_cfg.max_epochs = 50
#     cfg.work_dir = WORK_DIR
# 
#     # Teacher ë¡œë“œ
#     if os.path.exists(TEACHER_PATH):
#         print(f"   âœ… Teacher ëª¨ë¸ ë¡œë“œë¨: {TEACHER_PATH}")
#         cfg.load_from = TEACHER_PATH
#     else:
#         print(f"   âš ï¸ Teacher ëª¨ë¸ ì—†ìŒ ({TEACHER_PATH}). ë‹¨ë… í•™ìŠµ ì§„í–‰.")
# 
#     # ì‹¤í–‰
#     runner = Runner.from_cfg(cfg)
#     runner.train()
# 
# if __name__ == "__main__":
#     step1_prepare_data()
#     step2_create_infos()
#     step3_train()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ ìš°ë¦¬ê°€ ë§Œë“  í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
!export NUMBA_DISABLE_CUDA=1 && \
 conda run -n openmmlab python -u run_all_in_one.py



"""# 21. ë‹¤ì¤‘ ìƒ˜í”Œ ë¹„êµ"""

# Commented out IPython magic to ensure Python compatibility.
# # ë‹¤ì¤‘ ìƒ˜í”Œ ë¹„êµ ì‹œê°í™”
# %%bash
# PY=/content/conda/envs/openmmlab/bin/python
# 
# $PY << 'EOF'
# import matplotlib
# matplotlib.use('Agg')
# 
# import numpy as np
# import matplotlib.pyplot as plt
# import pickle
# import os
# 
# print("=" * 70)
# print("ğŸ“Š ë‹¤ì¤‘ ìƒ˜í”Œ Detection ë¹„êµ")
# print("=" * 70)
# 
# LOCAL_KITTI = '/content/data/kitti'
# 
# with open(f'{LOCAL_KITTI}/kitti_infos_train.pkl', 'rb') as f:
#     data = pickle.load(f)
# infos = data['data_list']
# 
# pred_results = {}
# if os.path.exists('/content/inference_results.pkl'):
#     with open('/content/inference_results.pkl', 'rb') as f:
#         pred_results = pickle.load(f)
# 
# SAMPLES = [infos[i] for i in range(min(3, len(infos)))]
# 
# def draw_box_bev(ax, bbox, color, linestyle='-'):
#     x, y, z, dx, dy, dz, yaw = bbox[:7]
#     corners = np.array([[-dx/2, -dy/2], [dx/2, -dy/2],
#                        [dx/2, dy/2], [-dx/2, dy/2], [-dx/2, -dy/2]])
#     cos_y, sin_y = np.cos(yaw), np.sin(yaw)
#     rot = np.array([[cos_y, -sin_y], [sin_y, cos_y]])
#     corners = corners @ rot.T + np.array([x, y])
#     ax.plot(corners[:, 0], corners[:, 1], color=color, linewidth=2, linestyle=linestyle)
# 
# fig, axes = plt.subplots(2, 3, figsize=(18, 10))
# colors = {0: 'blue', 1: 'green', 2: 'red'}
# 
# for idx, sample_info in enumerate(SAMPLES):
#     sample_idx = sample_info['sample_idx']
#     sample_idx_str = f'{sample_idx:06d}'
# 
#     velodyne_file = f'{LOCAL_KITTI}/training/velodyne/{sample_idx_str}.bin'
#     points = np.fromfile(velodyne_file, dtype=np.float32).reshape(-1, 4)
# 
#     mask = (points[:, 0] > 0) & (points[:, 0] < 70) & (np.abs(points[:, 1]) < 40)
#     pts = points[mask]
# 
#     ax_gt = axes[0, idx]
#     ax_gt.scatter(pts[::3, 0], pts[::3, 1], s=0.1, c='gray', alpha=0.3)
#     gt_count = 0
#     if 'instances' in sample_info:
#         for inst in sample_info['instances']:
#             draw_box_bev(ax_gt, inst['bbox_3d'], colors.get(inst['bbox_label_3d'], 'yellow'))
#             gt_count += 1
#     ax_gt.set_title(f"GT - {sample_idx} ({gt_count} obj)", fontweight='bold')
#     ax_gt.set_xlim([0, 70])
#     ax_gt.set_ylim([-40, 40])
#     ax_gt.set_aspect('equal')
#     ax_gt.grid(True, alpha=0.3)
# 
#     ax_pred = axes[1, idx]
#     ax_pred.scatter(pts[::3, 0], pts[::3, 1], s=0.1, c='gray', alpha=0.3)
#     pred_count = 0
#     if sample_idx_str in pred_results:
#         pred_data = pred_results[sample_idx_str]
#         for i in range(len(pred_data['bboxes_3d'])):
#             if pred_data['scores_3d'][i] > 0.3:
#                 draw_box_bev(ax_pred, pred_data['bboxes_3d'][i],
#                             colors.get(pred_data['labels_3d'][i], 'yellow'))
#                 pred_count += 1
#     ax_pred.set_title(f"Pred - {sample_idx} ({pred_count} obj)", fontweight='bold')
#     ax_pred.set_xlim([0, 70])
#     ax_pred.set_ylim([-40, 40])
#     ax_pred.set_aspect('equal')
#     ax_pred.grid(True, alpha=0.3)
# 
#     print(f"   {sample_idx}: GT={gt_count}, Pred={pred_count}")
# 
# from matplotlib.lines import Line2D
# legend_elements = [
#     Line2D([0], [0], color='blue', linewidth=2, label='Pedestrian'),
#     Line2D([0], [0], color='green', linewidth=2, label='Cyclist'),
#     Line2D([0], [0], color='red', linewidth=2, label='Car'),
# ]
# fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=10)
# 
# plt.suptitle("KITTI Multi-Sample Comparison\n(Top: Ground Truth, Bottom: Prediction)",
#              fontsize=14, fontweight='bold')
# plt.tight_layout()
# plt.savefig('/content/multi_sample_comparison.png', dpi=150, bbox_inches='tight')
# plt.close()
# 
# print(f"\nğŸ’¾ ì €ì¥ë¨: /content/multi_sample_comparison.png")
# print("=" * 70)
# EOF

# ë‹¤ì¤‘ ìƒ˜í”Œ ì‹œê°í™” ì´ë¯¸ì§€ í‘œì‹œ
from IPython.display import Image, display
display(Image('/content/multi_sample_comparison.png'))